\section{Numéro 2}

\subsection{Modification dans le code}
    Un import supplémentaire a été invoqué afin de simplifier les opérations d'augmentation de données:\\
    
    Le code suivant a été ajouté pour définir le transformateur des données d'entraînement: \\
    
    La commande \textit{RandomRotation} permet d'appliquer une rotation aléatoire entre 0 degré et la valeur passée en paramètre. \textit{ColorJitter} permet d'ajouter une variance aléatoire sur la couleur des images. \textit{RandomHorizontalFlip} permet d'ajouter d'effectuer une inversion horizontale selon une probabilité passée en paramètre. Finalement, \textit{RandomResizedCrop} permet d'appliquer un recadrage aléatoire sur la photo et un redimensionnement selon le paramètre passé.\\
    
    Les jeux de données d'entraînement ont été chargés en spécifiant le transformateur défini précédemment:    
    
    Ces transformateurs sont utilisés pour valider l'effet de l'augmentation de données sur la justesse d'apprentissage sur le jeu d'entraînement.
    
\subsection{Résultat des entraînement du réseau CNN vanilla}
    Comme attendu, les résultats de l'entraînement sans augmentation de données causent un surentraînement évident. On voit également que la justesse de la courbe de validation a stagné à partir des environs de la dixième époque.
    \begin{figure}[H]
        \centering 
        %\includegraphics[]{}
        \caption{Résultat de l'entraînement sur cifar10 du CNN vanilla sans augmentation de données}
    \end{figure}

\pagebreak
\section{Numéro 4}
\subsection{Augmentation des données}
    Pour réaliser l'augmentation des données, nous avons créé la classe \texttt{AcdcDataAugmentationTransform} présent dans le ficher \texttt{acdc\_transform.py}. Cette classe applique les transformations de couleur, de rotation et de rognage aux images cardiaques et applique les transformations de rotation et de rognage à la vérité terrain.\\
    
    Le code suivant a été ajouté pour définir le transformateur des données d'entraînement: \\
    
    Les jeux de données d'entraînement ont été chargés en spécifiant le transformateur défini précédemment:    

%\begin{landscape}
    \subsection{Architecture}
        À la fin du réseau, il y a un \textit{softmax}. Il n'est pas dans le schéma, car il n'est pas dans la classe du réseau. \(N_A\) correspond à la profondeur du UNET en ne comptant pas le blob de transition (nombre de \textit{MaxPool} ou nombre de \textit{ConvTranspose}). \(N_B\) correspond au nombre de répétitions des sous-ensembles de blocs constitués d'un bloc dense, suivi d'un bloc résiduel. Ces sous-ensembles de blocs composent les blocs de contraction et les blocs d'expansion. La définition du bloc dense et du bloc résiduel est donnée à la section précédente.
        \begin{figure}[H]
            \centering 
            %\includegraphics[]{}
            \caption{Architecture de IFT725\_UNET}
        \end{figure}
%\end{landscape}

\subsection{Résultats de la recherche d’hyperparamètres}
    Le tableau suivant présente les résultats de la recherche d'hyperparamètres. La meilleure configuration obtient une justesse de validation de 85,4 \% (\(L_R=0,001\), \(N_A=5\) \(N_B=1\) \(A_d =\) Non). Il est aussi possible d'affirmer que augmenter la valeur de \(N_B\) réduit la justesse de validation et qu'augmenter \(N_A\) augmente la justesse de validation. Lorsque l'augmentation des données est activée, le meilleur taux d'apprentissage est de 0,002, tandis que le meilleur taux d'apprentissage est de 0,001 sans l'augmentation des données. Normalement, l'augmentation des données devrait améliorer la justesse de validation, mais ce n'est pas le cas dans nos résultats. Ceci est expliqué à la section suivante.
    
    \begin{longtable}{p{4cm}p{1cm}p{1cm}p{3.5cm}p{4cm}}
        \caption{Résultat de la recherche des hyperparamètres}\\
        \hline
        Taux d'apprentissage (\(L_R\)) & \(N_A\) & \(N_B\) & Augmentation des données (\(A_d\)) & Justesse de validation (\%) \\
        \hline\hline
        0.001 & 3 & 1 & Non & 84,8\\
        0.001 & 3 & 2 & Non & 84,3\\
        0.001 & 3 & 3 & Non & 83.1\\
        
        0,001 & 4 & 1 & Non & 85,1\\
        0,001 & 4 & 2 & Non & 84,1\\
        0,001 & 4 & 3 & Non & - \\
        
        \textbf{0,001} & \textbf{5} & \textbf{1} & \textbf{Non} & \textbf{85,4}\\
        0,001 & 5 & 2 & Non & 84,4\\
        0,001 & 5 & 3 & Non & 84,0\\
        
        \hline
    \end{longtable}

\subsection{Courbes d'apprentissage}
    À partir des figures suivantes, il est possible de conclure que l'augmentation des données permet d'enlever le léger surentraînement, mais cause un entraînement bien plus long. À l'aide de la tendance des courbes, les résultats avec l'augmentation des données auraient été meilleurs que ceux sans augmentation des données si l'entraînement avait duré plus longtemps.

    \begin{figure}[H]
        \centering 
        %\includegraphics[]{}
        \caption{Courbes d’apprentissage - \(L_R=0.001\), \(N_A=3\), \(N_B=1\), \(A_d= \) Non}
    \end{figure}
